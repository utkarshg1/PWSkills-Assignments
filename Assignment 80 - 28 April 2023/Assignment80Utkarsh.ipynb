{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Assignment  80: Clustering 2 - Utkarsh Gaikwad</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Assignment pdf link](./28%20Apr_AssQ.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 1</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : What is hierarchical clustering, and how is it different from other clustering techniques?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering is a type of clustering algorithm that groups similar data points into clusters based on their similarity, creating a hierarchy of clusters. This method creates a dendrogram, a tree-like diagram, to visualize the clusters at different levels of the hierarchy.\n",
    "\n",
    "### There are two main types of hierarchical clustering: agglomerative and divisive. Agglomerative clustering starts with each data point as its own cluster and then iteratively merges the two most similar clusters until all data points belong to a single cluster. Divisive clustering, on the other hand, starts with all data points in a single cluster and then recursively splits the cluster into smaller clusters until each data point belongs to its own cluster.\n",
    "\n",
    "### Compared to other clustering techniques such as k-means or DBSCAN, hierarchical clustering has the advantage of not requiring a pre-specified number of clusters. It can also reveal the hierarchical relationships between the clusters, which can be useful in certain applications. However, hierarchical clustering can be computationally expensive, especially for large datasets, and the choice of distance metric and linkage method can have a significant impact on the resulting clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 2</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: What are the two main types of hierarchical clustering algorithms? Describe each in brief.\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The two main types of hierarchical clustering algorithms are agglomerative clustering and divisive clustering.\n",
    "\n",
    "### Agglomerative clustering, also known as bottom-up clustering, starts by considering each data point as a separate cluster and then recursively merges the two closest clusters until all data points belong to a single cluster. This process is often visualized as a dendrogram, which shows the hierarchy of clusters as the algorithm progresses. At each step of the algorithm, the two closest clusters are merged based on some distance metric between them, such as Euclidean distance or correlation. The choice of distance metric and linkage method (which determines how to compute the distance between clusters) can significantly impact the resulting clusters.\n",
    "\n",
    "### Divisive clustering, also known as top-down clustering, starts by considering all data points as belonging to a single cluster and then recursively divides the cluster into smaller subclusters until each data point belongs to its own cluster. This process is also often visualized as a dendrogram, but with the leaves representing individual data points instead of clusters. Divisive clustering is less commonly used than agglomerative clustering because it can be computationally expensive and sensitive to the initial choice of clustering.\n",
    "\n",
    "### Both types of hierarchical clustering have their strengths and weaknesses. Agglomerative clustering is more commonly used and can reveal the hierarchical relationships between clusters, but it can be computationally expensive and sensitive to the choice of distance metric and linkage method. Divisive clustering is less commonly used and can be computationally expensive, but it can reveal the most significant splits in the data early in the process. The choice of which type of clustering to use depends on the specific requirements of the problem at hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 3</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : How do you determine the distance between two clusters in hierarchical clustering, and what are the common distance metrics used?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In hierarchical clustering, the distance between two clusters is determined by a distance metric that measures the similarity or dissimilarity between the observations or data points in the clusters. The choice of distance metric can have a significant impact on the resulting clusters.\n",
    "\n",
    "### There are several common distance metrics used in hierarchical clustering:\n",
    "\n",
    "1. `Euclidean distance`: This is the most commonly used distance metric in clustering. It is calculated as the square root of the sum of the squared differences between the corresponding features of two data points. It assumes that the features are on the same scale and that the differences between them are normally distributed.\n",
    "\n",
    "2. `Manhattan distance`: Also known as city block distance, this metric measures the distance between two data points as the sum of the absolute differences between the corresponding features. It is useful when the features are not on the same scale.\n",
    "\n",
    "3. `Cosine distance`: This metric measures the cosine of the angle between two vectors in a high-dimensional space. It is commonly used in text clustering and other applications where the data points are represented as high-dimensional vectors.\n",
    "\n",
    "4. `Correlation distance`: This metric measures the similarity between two data points by computing their correlation coefficient. It is useful when the data points have different scales or when the differences between features are not normally distributed.\n",
    "\n",
    "5. `Hamming distance`: This metric is used when the data points are binary vectors, where each feature can take on one of two values (0 or 1). It measures the number of positions in which two vectors differ.\n",
    "\n",
    "### There are many other distance metrics that can be used in hierarchical clustering, depending on the specific problem and the characteristics of the data. The choice of distance metric can have a significant impact on the resulting clusters, so it is important to carefully consider which metric to use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 4</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : How do you determine the optimal number of clusters in hierarchical clustering, and what are some common methods used for this purpose?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the optimal number of clusters in hierarchical clustering is an important but challenging task. There is no one-size-fits-all method for determining the optimal number of clusters, as it depends on the specific problem and data at hand. However, there are several common methods used for this purpose:\n",
    "\n",
    "1. `Dendrogram`: The dendrogram is a tree-like diagram that shows the hierarchy of clusters as the algorithm progresses. By visually inspecting the dendrogram, one can identify the natural clusters at different levels of the hierarchy. However, this method is subjective and requires human judgment.\n",
    "\n",
    "2. `Elbow method`: The elbow method involves plotting the within-cluster sum of squares (WSS) against the number of clusters and selecting the number of clusters at the \"elbow\" of the plot, where the decrease in WSS starts to level off. This method is commonly used in k-means clustering but can also be used in hierarchical clustering by computing the WSS for each level of the hierarchy.\n",
    "\n",
    "3. `Silhouette method`: The silhouette method measures how well each data point fits into its assigned cluster compared to other clusters. It computes a silhouette score for each data point, which ranges from -1 to 1, with higher scores indicating better clustering. The overall silhouette score for a given number of clusters can be used to select the optimal number of clusters.\n",
    "\n",
    "4. `Gap statistic`: The gap statistic compares the within-cluster dispersion for different numbers of clusters to that of a reference distribution generated by a null model. The optimal number of clusters is selected as the number that maximizes the gap between the observed dispersion and the expected dispersion under the null model.\n",
    "\n",
    "5. `Hierarchical clustering metrics`: There are several metrics that can be used to evaluate the quality of hierarchical clustering, such as cophenetic correlation coefficient, silhouette coefficient, and Dunn index. These metrics can be used to compare the clustering results for different numbers of clusters and select the optimal number.\n",
    "\n",
    "### In practice, it is common to use a combination of these methods to select the optimal number of clusters. However, it is important to keep in mind that the choice of the optimal number of clusters is not always clear-cut and may require some trial and error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 5</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In hierarchical clustering, a dendrogram is a graphical representation of the results that displays the hierarchy of clusters as a tree-like diagram. Dendrograms are useful in analyzing the results of hierarchical clustering because they provide a visual representation of the relationships between the clusters and the data points.\n",
    "\n",
    "### Each leaf node in the dendrogram represents a data point, and the branches represent the clusters formed at each level of the hierarchy. The height of each branch represents the distance between the clusters at that level, with longer branches indicating greater distance. The root of the dendrogram represents the entire dataset, and the leaves represent the individual data points.\n",
    "\n",
    "### Dendrograms can be used to identify natural clusters in the data at different levels of the hierarchy. By visually inspecting the dendrogram, one can identify the clusters that are formed by cutting the tree at different heights. This can be particularly useful when the optimal number of clusters is not clear-cut, as it allows for a more nuanced understanding of the relationships between the data points.\n",
    "\n",
    "### Dendrograms can also be used to detect outliers in the data. Outliers are data points that are very dissimilar to all other data points and may appear as isolated branches in the dendrogram. By identifying these outliers, one can gain insights into the underlying patterns in the data and potentially remove them from further analysis.\n",
    "\n",
    "### Overall, dendrograms are a useful tool for visualizing the results of hierarchical clustering and gaining insights into the relationships between the data points and clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dendogram](https://res.cloudinary.com/dyd911kmh/image/upload/v1674149819/Dendrogram_of_Agglomerative_Clustering_Approach_4eba3586ec.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 6</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 : Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the distance metrics different for each type of data?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yes, hierarchical clustering can be used for both numerical and categorical data. However, the distance metrics used for each type of data are different.\n",
    "\n",
    "### For numerical data, common distance metrics include Euclidean distance, Manhattan distance, and cosine similarity. Euclidean distance measures the straight-line distance between two points in a multi-dimensional space, while Manhattan distance measures the distance along the axes. Cosine similarity measures the cosine of the angle between two vectors, and is often used in text clustering.\n",
    "\n",
    "### For categorical data, common distance metrics include Jaccard distance, Hamming distance, and Gower distance. Jaccard distance measures the dissimilarity between two sets, and is often used for binary categorical data. Hamming distance measures the number of positions at which two strings of equal length are different, and is often used for categorical data with multiple categories. Gower distance is a more general distance metric that can be used for mixed data types, such as a combination of numerical and categorical data.\n",
    "\n",
    "### It is worth noting that some clustering algorithms can handle mixed data types directly, such as the K-Prototypes algorithm which extends K-Means to handle mixed data types. However, hierarchical clustering typically requires a distance metric that can handle the specific data type, which may require pre-processing the data to convert it into a suitable format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 7</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 : How can you use hierarchical clustering to identify outliers or anomalies in your data?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering can be used to identify outliers or anomalies in data by identifying data points that are dissimilar from all other data points. These data points will appear as isolated branches or clusters in the dendrogram.\n",
    "\n",
    "### One way to identify outliers is to use the height of the branches in the dendrogram as a measure of dissimilarity. Outliers are data points that have a large dissimilarity to all other data points, and thus will be located at the bottom of the dendrogram. One can visually inspect the dendrogram to identify branches with low density or isolated clusters that are separate from the main clusters.\n",
    "\n",
    "### Another way to identify outliers is to use a clustering algorithm that allows for the formation of singleton clusters. A singleton cluster is a cluster with only one data point. By setting a threshold on the minimum cluster size, one can identify singleton clusters, which represent potential outliers. These singleton clusters can then be inspected manually to determine if they are indeed outliers or if they represent legitimate data points.\n",
    "\n",
    "### Once potential outliers have been identified, it is important to further investigate them to determine if they are indeed anomalies or if they represent legitimate data points. This can involve further data cleaning, pre-processing, or statistical analysis to understand the nature of the outlier and its potential impact on the analysis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
