{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Assignment  76: Dimensionality Reduction 2 - Utkarsh Gaikwad</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Assignment pdf link](./24%20Apr_AssQ.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 1</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : What is a projection and how is it used in PCA?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In PCA (Principal Component Analysis), a projection is a way to transform the data from the original high-dimensional space to a lower-dimensional space. The projection involves identifying the directions of maximum variance in the data and projecting the data onto these directions.\n",
    "\n",
    "### To compute the projection, PCA first calculates the eigenvectors and eigenvalues of the covariance matrix of the data. The eigenvectors represent the directions of maximum variance in the data, while the eigenvalues represent the amount of variance explained by each eigenvector. The eigenvectors are sorted in descending order based on their corresponding eigenvalues, and the top k eigenvectors are selected to define the k-dimensional subspace onto which the data will be projected.\n",
    "\n",
    "### To project the data onto the subspace, the data is first centered by subtracting the mean of each feature. Then, the centered data is multiplied by the matrix of the top k eigenvectors to obtain the projected data in the lower-dimensional subspace.\n",
    "\n",
    "### The projected data represents a compressed version of the original data, where the dimensions with low variance are discarded. By choosing the number of dimensions k, the amount of information retained in the projected data can be controlled.\n",
    "\n",
    "### Overall, the projection step in PCA is a powerful technique for dimensionality reduction, which allows the most important information in the data to be retained while reducing the number of dimensions required to represent the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 2</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 : How does the optimization problem in PCA work, and what is it trying to achieve?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis) is a statistical technique that aims to reduce the dimensionality of a dataset by identifying the most important features that explain the most variation in the data. The optimization problem in PCA is to find a set of orthogonal basis vectors that can best represent the data in terms of maximizing the variance explained.\n",
    "\n",
    "### In other words, PCA tries to find a linear transformation of the data that reduces the dimensionality while preserving the maximum amount of information. This is done by identifying the principal components, which are the directions in which the data has the highest variance. The first principal component corresponds to the direction with the highest variance, and each subsequent component corresponds to the next highest variance, subject to the constraint that the components are orthogonal to each other.\n",
    "\n",
    "### The optimization problem in PCA can be formulated as an eigenvalue problem, where the principal components are the eigenvectors of the covariance matrix of the data, and the eigenvalues represent the amount of variance explained by each component. The objective is to find the eigenvectors that correspond to the largest eigenvalues, which represent the most important directions of variation in the data.\n",
    "\n",
    "### Once the principal components have been identified, they can be used to project the original data onto a lower-dimensional subspace, where each data point is represented as a linear combination of the principal components. This can be useful for visualization, data compression, and feature extraction.\n",
    "\n",
    "### In summary, the optimization problem in PCA is trying to find the most informative representation of the data by identifying the directions of highest variance and projecting the data onto a lower-dimensional subspace while preserving as much information as possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PCA](https://alexhwilliams.info/itsneuronalblog/img/pca/pca_classic.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 3</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : What is the relationship between covariance matrices and PCA?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance matrices and PCA (Principal Component Analysis) are closely related, as the covariance matrix is a fundamental part of the PCA algorithm.\n",
    "\n",
    "### In PCA, the goal is to identify the principal components, which are the directions of highest variance in the data. To do this, we first calculate the covariance matrix of the data, which is a matrix that describes the pairwise relationships between the variables in the data. Specifically, the covariance matrix tells us how much two variables vary together, or how much one variable varies as another variable is held constant.\n",
    "\n",
    "### The covariance matrix is a square matrix where the diagonal entries represent the variances of each variable, and the off-diagonal entries represent the covariances between the variables. The diagonal entries tell us how much each variable varies on its own, while the off-diagonal entries tell us how much each variable varies with respect to the other variables.\n",
    "\n",
    "### PCA uses the covariance matrix to find the principal components, which are the eigenvectors of the covariance matrix. The eigenvectors represent the directions of highest variance in the data, and the corresponding eigenvalues represent the amount of variance explained by each component. By finding the eigenvectors with the highest eigenvalues, PCA identifies the most informative directions of variation in the data.\n",
    "\n",
    "### Once the principal components have been identified, they can be used to transform the original data into a new coordinate system, where each data point is represented as a linear combination of the principal components. This new representation can be used for visualization, dimensionality reduction, or feature extraction.\n",
    "\n",
    "### In summary, the covariance matrix is used in PCA to identify the principal components, which are the directions of highest variance in the data. The eigenvectors of the covariance matrix represent the principal components, and the eigenvalues represent the amount of variance explained by each component."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 4</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : How does the choice of number of principal components impact the performance of PCA?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The choice of the number of principal components in PCA (Principal Component Analysis) can have a significant impact on the performance of the algorithm. The number of principal components determines the dimensionality of the transformed data, and can affect the quality of the results obtained.\n",
    "\n",
    "### If too few principal components are chosen, the transformed data may not capture all of the important information in the original data. This can lead to a loss of information and a decrease in the quality of the results. On the other hand, if too many principal components are chosen, the transformed data may include noise and irrelevant information, which can also decrease the quality of the results.\n",
    "\n",
    "### In general, the choice of the number of principal components depends on the specific application and the trade-off between information retention and dimensionality reduction. One common approach is to choose the number of principal components that capture a certain percentage of the total variance in the data. For example, if we want to retain 95% of the variance in the data, we can choose the number of principal components such that the sum of their corresponding eigenvalues is equal to 95% of the total sum of the eigenvalues.\n",
    "\n",
    "### Another approach is to use cross-validation or other performance metrics to select the optimal number of principal components for a particular task. This involves evaluating the performance of the PCA algorithm on a test set for different values of the number of principal components and choosing the value that maximizes the performance metric.\n",
    "\n",
    "### In summary, the choice of the number of principal components in PCA depends on the specific application and the trade-off between information retention and dimensionality reduction. It is important to choose an appropriate number of principal components to ensure that the transformed data captures the important information in the original data while reducing its dimensionality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 5</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 : How can PCA be used in feature selection, and what are the benefits of using it for this purpose?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis) can be used in feature selection to identify the most informative features in a dataset. The benefits of using PCA for feature selection include reducing the dimensionality of the data, identifying the most informative features, and removing correlated features.\n",
    "\n",
    "### One way to use PCA for feature selection is to calculate the principal components of the data and select the top N components that explain the most variance. The corresponding eigenvectors can then be used as a set of features that captures the most important information in the original data. This approach can be useful when dealing with high-dimensional data, where the number of features is much larger than the number of samples.\n",
    "\n",
    "### Another way to use PCA for feature selection is to calculate the loadings of the features on the principal components and select the features with the highest loadings. The loadings represent the correlations between the features and the principal components, and can be used to identify the most informative features. This approach can be useful when the number of features is small and the focus is on selecting a subset of features that capture the most important information.\n",
    "\n",
    "### PCA can also be used to remove correlated features, which can improve the performance of machine learning algorithms. Correlated features can introduce redundancy and increase the dimensionality of the data, which can lead to overfitting and decreased performance. By identifying the principal components that explain the most variance, PCA can remove correlated features and reduce the dimensionality of the data, which can improve the performance of machine learning algorithms.\n",
    "\n",
    "### In summary, PCA can be used in feature selection to reduce the dimensionality of the data, identify the most informative features, and remove correlated features. The benefits of using PCA for feature selection include improving the performance of machine learning algorithms, reducing the risk of overfitting, and improving the interpretability of the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 6</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 : What are some common applications of PCA in data science and machine learning?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis) is a widely used unsupervised machine learning technique for reducing the dimensionality of a dataset. Here are some common applications of PCA in data science and machine learning:\n",
    "\n",
    "1. `Data Visualization`: PCA can be used to visualize high-dimensional data in two or three dimensions, which makes it easier to identify patterns and relationships in the data.\n",
    "\n",
    "2. `Feature Extraction`: PCA can be used to extract the most important features from a dataset, which can be used as input for other machine learning algorithms.\n",
    "\n",
    "3. `Data Compression`: PCA can be used to compress large datasets by reducing the number of features, without losing too much information.\n",
    "\n",
    "4. `Data Preprocessing`: PCA can be used to preprocess data by reducing noise, removing redundant information, and normalizing the data.\n",
    "\n",
    "5. `Image Processing`: PCA can be used to reduce the dimensionality of image data, making it easier to analyze and process.\n",
    "\n",
    "6. `Anomaly Detection`: PCA can be used to detect anomalies in data by comparing new data points to the principal components of the training data.\n",
    "\n",
    "7. `Clustering`: PCA can be used to cluster similar data points together based on their principal components.\n",
    "\n",
    "### Overall, PCA is a versatile technique that can be used in a wide range of applications in data science and machine learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 7</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 : What is the relationship between spread and variance in PCA?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In PCA, spread and variance are closely related concepts.\n",
    "\n",
    "### Spread refers to the extent to which the data points in a dataset are distributed or dispersed, and it can be measured using metrics such as range, interquartile range, or standard deviation. Variance, on the other hand, is a measure of how much the data points in a dataset vary or deviate from their mean.\n",
    "\n",
    "### In PCA, the principal components are defined as the linear combinations of the original features that capture the maximum amount of variance in the dataset. Therefore, the spread of the dataset is related to the variance of the principal components. Specifically, the spread of the data along a particular principal component is proportional to the variance of that component.\n",
    "\n",
    "### In other words, if a principal component has a high variance, it means that the data points are spread out along that component, and vice versa. This relationship between spread and variance is what makes PCA a useful tool for reducing the dimensionality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 8</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 : How does PCA use the spread and variance of the data to identify principal components?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis) is a statistical technique that aims to identify the underlying patterns in a dataset by reducing the number of variables while retaining the maximum amount of variance in the data.\n",
    "\n",
    "### PCA achieves this by finding the principal components of the dataset, which are linear combinations of the original variables that capture the most variance in the data. The first principal component is the direction in which the data varies the most, and each subsequent principal component captures the remaining variance in orthogonal directions.\n",
    "\n",
    "### To identify the principal components, PCA uses the spread and variance of the data. The spread of the data is measured by the covariance matrix, which shows how the variables in the dataset are related to each other. The variance of each variable is also calculated to determine its contribution to the overall spread of the data.\n",
    "\n",
    "### PCA then identifies the principal components by finding the eigenvectors of the covariance matrix. The eigenvectors are the directions in which the data varies the most, and the corresponding eigenvalues represent the amount of variance captured in each direction.\n",
    "\n",
    "### By selecting the eigenvectors with the highest eigenvalues, PCA identifies the principal components that capture the most variance in the data. These principal components can then be used to reduce the dimensionality of the dataset while retaining as much information as possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 9</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 : How does PCA handle data with high variance in some dimensions but low variance in others?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis) is a powerful technique for analyzing data and identifying underlying patterns by reducing the dimensionality of the data while retaining the maximum amount of variance. When some dimensions of the data have high variance while others have low variance, PCA is still able to effectively identify the principal components that capture the most variance in the data.\n",
    "\n",
    "### In such cases, PCA identifies the principal components based on the overall spread of the data, rather than just the variance in individual dimensions. The principal components are linear combinations of the original variables that capture the most variance in the data, regardless of which dimensions have high or low variance.\n",
    "\n",
    "### Specifically, the principal components are found by computing the eigenvectors of the covariance matrix of the data. The covariance matrix takes into account the variance of each variable as well as the covariance between all pairs of variables, which enables PCA to capture the overall spread of the data.\n",
    "\n",
    "### When some dimensions of the data have high variance and others have low variance, the covariance matrix will reflect this and the resulting eigenvectors will capture the directions of maximum variance in the data, regardless of which dimensions have high or low variance.\n",
    "\n",
    "### Therefore, PCA is able to handle data with high variance in some dimensions but low variance in others by identifying the principal components based on the overall spread of the data, rather than just the variance in individual dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
