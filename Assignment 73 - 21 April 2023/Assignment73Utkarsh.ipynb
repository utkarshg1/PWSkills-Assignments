{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Assignment  73: KNN 2 - Utkarsh Gaikwad</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Assignment pdf link](./21%20Apr_AssQ.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 1</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main difference between the Euclidean distance metric and the Manhattan distance metric in KNN is how they measure the distance between two data points.\n",
    "\n",
    "### Euclidean distance measures the straight-line distance between two points in a multi-dimensional space, similar to calculating the distance between two points on a map. Mathematically, it is calculated as the square root of the sum of the squared differences between the corresponding coordinates of the two points.\n",
    "\n",
    "### Manhattan distance, also known as taxicab distance or L1 distance, measures the distance between two points by summing the absolute differences between their corresponding coordinates. It is called taxicab distance because it's like measuring the distance between two points in a city by following the grid-like pattern of the streets, as a taxicab would do.\n",
    "\n",
    "### The choice of distance metric can affect the performance of a KNN classifier or regressor, as it determines how \"close\" or \"similar\" two data points are considered to be. Euclidean distance tends to work well when the differences between the values in the different dimensions are important and the data is continuous. On the other hand, Manhattan distance can work well when the dimensions represent categorical or binary data, and when the differences in the values of different dimensions are equally important.\n",
    "\n",
    "### In some cases, one metric may perform better than the other depending on the nature of the data and the problem at hand. Therefore, it's often a good idea to try both distance metrics and evaluate the performance of the KNN model using cross-validation or other evaluation metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 2</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 : How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the optimal value of k for a KNN classifier or regressor is important for achieving good performance on a given dataset. The choice of k can have a significant impact on the accuracy, precision, and recall of the KNN model.\n",
    "\n",
    "### One approach to determine the optimal k value is to use a validation set or cross-validation to evaluate the performance of the KNN model for different k values. This involves splitting the dataset into a training set and a validation set, and training the KNN model with different k values on the training set, and then evaluating the performance of the model on the validation set. This process can be repeated multiple times with different splits of the data, and the average performance can be used to select the best k value.\n",
    "\n",
    "### Another approach is to use a grid search or random search over a range of k values, and evaluate the performance of the model using a performance metric such as accuracy, F1-score, or mean squared error. This involves training the KNN model with different k values on the entire dataset and selecting the k value that gives the best performance on the validation set or through cross-validation.\n",
    "\n",
    "### In addition to these techniques, it is also important to consider the size of the dataset, the number of features, and the nature of the data when selecting the k value. A larger k value can lead to smoother decision boundaries, but can also lead to over-generalization and poor performance on small datasets. A smaller k value can lead to over-fitting and poor performance on noisy or sparse datasets.\n",
    "\n",
    "### Overall, the choice of the optimal k value for a KNN classifier or regressor is problem-specific and requires experimentation and evaluation using different techniques and performance metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 3</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The choice of distance metric can have a significant impact on the performance of a KNN classifier or regressor. Different distance metrics measure the distance or similarity between data points in different ways, which can affect the accuracy and reliability of the KNN model.\n",
    "\n",
    "### For example, Euclidean distance measures the straight-line distance between two points in a multi-dimensional space. It works well when the differences between the values in the different dimensions are important and the data is continuous. On the other hand, Manhattan distance measures the distance between two points by summing the absolute differences between their corresponding coordinates. It can work well when the dimensions represent categorical or binary data, and when the differences in the values of different dimensions are equally important.\n",
    "\n",
    "### In general, if the data is continuous and the differences between the values in the different dimensions are important, Euclidean distance may be a good choice. On the other hand, if the data is categorical or binary and the differences in the values of different dimensions are equally important, Manhattan distance may be a better choice.\n",
    "\n",
    "### However, there is no one-size-fits-all answer, and the choice of distance metric should depend on the nature of the data and the problem at hand. In some cases, other distance metrics such as Minkowski distance or Mahalanobis distance may be more appropriate.\n",
    "\n",
    "### It is often a good idea to experiment with different distance metrics and evaluate the performance of the KNN model using cross-validation or other evaluation metrics to select the best distance metric for a given problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 4</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are several hyperparameters in KNN classifiers and regressors that can affect the performance of the model. Some common hyperparameters include:\n",
    "\n",
    "1. `k`: The number of neighbors used for classification or regression. A larger k value can lead to smoother decision boundaries, but can also lead to over-generalization and poor performance on small datasets. A smaller k value can lead to over-fitting and poor performance on noisy or sparse datasets.\n",
    "\n",
    "2. `Distance metric`: The distance metric used to measure the distance or similarity between data points. Different distance metrics can be more or less appropriate depending on the nature of the data and the problem at hand.\n",
    "\n",
    "3. `Weighting scheme`: The weighting scheme used to give more or less weight to the neighbors depending on their distance from the query point. Uniform weighting gives equal weight to all neighbors, while distance-weighted or kernel-weighted schemes give more weight to closer neighbors.\n",
    "\n",
    "4. `Leaf size`: The maximum number of points in a leaf node of the KD-tree or ball tree data structure used to speed up the nearest neighbor search. A larger leaf size can lead to faster queries but can also lead to lower accuracy, while a smaller leaf size can lead to higher accuracy but longer query times.\n",
    "\n",
    "### To tune these hyperparameters and improve model performance, one approach is to use grid search or random search over a range of hyperparameters values and evaluate the performance of the model using cross-validation or other evaluation metrics. This involves training the KNN model with different hyperparameter values on the training set, and then evaluating the performance of the model on the validation set. This process can be repeated multiple times with different splits of the data, and the average performance can be used to select the best hyperparameters.\n",
    "\n",
    "### Another approach is to use more advanced techniques such as Bayesian optimization or gradient-based optimization to find the optimal hyperparameters. These techniques can be more computationally expensive but can potentially find better hyperparameter values.\n",
    "\n",
    "### It is important to note that the optimal hyperparameters for a KNN model may depend on the specific dataset and problem at hand, so it is important to experiment with different hyperparameters and evaluate the performance of the model using cross-validation or other evaluation metrics to find the best hyperparameters for a given problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 5</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 : How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The size of the training set can have a significant impact on the performance of a KNN classifier or regressor. A smaller training set can lead to overfitting, where the model is too complex and fits the noise in the data rather than the underlying patterns, while a larger training set can lead to underfitting, where the model is too simple and does not capture the underlying patterns in the data.\n",
    "\n",
    "### Generally, increasing the size of the training set can improve the performance of the KNN model, up to a certain point. However, beyond a certain point, adding more data may not necessarily improve the performance of the model and can even lead to diminishing returns or increased computational cost.\n",
    "\n",
    "### To optimize the size of the training set, one approach is to use cross-validation to evaluate the performance of the model on different subsets of the data. This involves splitting the data into training and validation sets and training the KNN model on the training set while evaluating its performance on the validation set. This process can be repeated multiple times with different splits of the data to get an estimate of the model's performance on unseen data. By varying the size of the training set, it is possible to determine the optimal size that maximizes the performance of the model on the validation set.\n",
    "\n",
    "### Another approach is to use techniques such as random subsampling or bootstrapping to generate multiple subsets of the data and train the KNN model on each subset. By varying the size of the subsets, it is possible to determine the optimal size that maximizes the performance of the model on the full dataset.\n",
    "\n",
    "### It is important to note that the optimal size of the training set may depend on the specific dataset and problem at hand, and may also depend on other factors such as the complexity of the model and the size of the feature space. Therefore, it is important to experiment with different training set sizes and evaluate the performance of the model using cross-validation or other evaluation metrics to determine the optimal size for a given problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 6</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 : What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### While KNN can be a simple and effective algorithm for classification or regression tasks, there are also some potential drawbacks to its use:\n",
    "\n",
    "1. `Computationally expensive`: KNN can be computationally expensive, especially when dealing with large datasets or high-dimensional feature spaces. This is because it requires computing the distances between each query point and all the training points, which can become computationally prohibitive as the size of the dataset grows.\n",
    "\n",
    "2. `Sensitivity to the choice of hyperparameters`: KNN performance can be sensitive to the choice of hyperparameters such as the number of neighbors (k) or the distance metric used. Selecting the optimal hyperparameters can be challenging, and different hyperparameter choices may be optimal for different datasets.\n",
    "\n",
    "3. `Imbalanced data`: KNN may not perform well on imbalanced datasets, where one class or target variable has much fewer examples than the other. This is because the majority class or target variable can dominate the decision-making process and lead to poor performance on the minority class or target variable.\n",
    "\n",
    "### To overcome these drawbacks and improve the performance of KNN, there are several strategies that can be employed:\n",
    "\n",
    "1. `Use approximate nearest neighbor methods`: To address the computational complexity of KNN, approximate nearest neighbor methods such as locality-sensitive hashing or randomized search trees can be used to speed up the nearest neighbor search.\n",
    "\n",
    "2. `Use feature selection or dimensionality reduction`: To reduce the size of the feature space and improve the performance of KNN, feature selection or dimensionality reduction techniques can be used to select a subset of the most informative features or to reduce the dimensionality of the feature space.\n",
    "\n",
    "3. `Use ensemble methods`: To improve the robustness and performance of KNN, ensemble methods such as bagging, boosting, or stacking can be used to combine multiple KNN models with different hyperparameters or training subsets.\n",
    "\n",
    "4. `Use resampling techniques`: To address the problem of imbalanced data, resampling techniques such as oversampling or undersampling can be used to balance the classes or target variables in the dataset.\n",
    "\n",
    "5. `Use cross-validation`: To select the optimal hyperparameters for KNN, cross-validation can be used to evaluate the performance of the model on different subsets of the data and to select the hyperparameters that lead to the best performance on unseen data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
